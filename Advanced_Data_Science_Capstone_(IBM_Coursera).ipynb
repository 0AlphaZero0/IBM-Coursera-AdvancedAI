{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced Data Science Capstone (IBM-Coursera)",
      "provenance": [],
      "authorship_tag": "ABX9TyPeaSBrqXqa6aR99aOFM5gX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0AlphaZero0/IBM-Coursera-AdvancedAI/blob/master/Advanced_Data_Science_Capstone_(IBM_Coursera).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWIPeyefLwVb"
      },
      "source": [
        "<h1>Fake news and Real news classification</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plK6HZNELF9F"
      },
      "source": [
        "*(Logistic Regression with pySpark and dataset from Kaggle)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvPmg5IAt5Cp"
      },
      "source": [
        "The kaggle dataset is \"[*Fake and real news dataset / Classifying the news*](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)\" by [**ClÃ©ment Bisaillon**](https://www.kaggle.com/clmentbisaillon)\n",
        "\n",
        "The goal here is to create a Spark machine learning algorithm to classify fake news and real news."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd_M5iuXLF22"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8alKFVGvB0m"
      },
      "source": [
        "First we need to setup the environment with **Kaggle credentials**. <br><br>Here in Google Colab you can do that with your credentials in google drive. First, download your kaggle credentials on your Kaggle Account and on the API section you can create a new API Token, it will download the file `kaggle.json`. You can either load your file each time or put it in a folder in your Google Drive and then requests it to be loaded.\n",
        "<br><br>\n",
        "This is the method used here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYCqYzvgLMU6"
      },
      "source": [
        "### Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6zls9A-ysuz"
      },
      "source": [
        "First, let's install kaggle!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzoz3F8uLLf6",
        "outputId": "57d135d9-6b1f-44de-e656-fbaf1baba70d"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ3xCYsbyx1i"
      },
      "source": [
        "Then thanks to the module `drive` of `google.colab`, let's connect to your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdh0F40tLG5D",
        "outputId": "3a5b9b16-cfef-4b32-95f9-3a412a981a3f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZzeiNwD1E5z"
      },
      "source": [
        "Now, we can get your `kaggle.json` and bring it in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGua7vNNLSrF",
        "outputId": "e587f5e2-723c-499b-a73a-a7039de96b71"
      },
      "source": [
        "!cp '/content/gdrive/My Drive/Credentials/kaggle.json' 'kaggle.json'\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json  # set permission"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35_u_j6piRqM"
      },
      "source": [
        "### Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNFL5eiv-pTm"
      },
      "source": [
        "Let's install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q0IJAlQiT5m",
        "outputId": "bd3370b3-93c1-4379-9fea-7f5b4a13e7d0"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212.4 MB 69 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198 kB 53.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=ba721cfa101773bf5b68a7f78368acaa60d946e99def84a7a98b51b43be100a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzDtPkQzLFw-"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-47L6xSK-zsm"
      },
      "source": [
        "To download the dataset we can do as followed :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8SJbOQaKrR6",
        "outputId": "24414c12-5ee2-4f65-f629-5cc0ee40fa02"
      },
      "source": [
        "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset # download dataset\n",
        "!mkdir fakenews # create a directory in Google Colab environment\n",
        "!unzip fake-and-real-news-dataset.zip -d fakenews/ # unzip the dataset in the directory previously created\n",
        "!rm fake-and-real-news-dataset.zip # remove the zip file"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading fake-and-real-news-dataset.zip to /content\n",
            " 85% 35.0M/41.0M [00:00<00:00, 80.1MB/s]\n",
            "100% 41.0M/41.0M [00:00<00:00, 92.1MB/s]\n",
            "Archive:  fake-and-real-news-dataset.zip\n",
            "  inflating: fakenews/Fake.csv       \n",
            "  inflating: fakenews/True.csv       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rysoBn4o_Y9w"
      },
      "source": [
        "Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BAOFKKdOSyv"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "from pyspark.sql.functions import col,lit,to_date"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bpgSH-4_iP4"
      },
      "source": [
        "Create Spark session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4KFJ-IdhPiK"
      },
      "source": [
        "spark = (SparkSession.builder\n",
        "                  .appName('Fake news detector')\n",
        "                  .enableHiveSupport()\n",
        "                  .config(\"spark.executor.memory\", \"4G\")\n",
        "                  .config(\"spark.driver.memory\",\"18G\")\n",
        "                  .config(\"spark.executor.cores\",\"7\")\n",
        "                  .config(\"spark.python.worker.memory\",\"4G\")\n",
        "                  .config(\"spark.driver.maxResultSize\",\"0\")\n",
        "                  .config(\"spark.sql.crossJoin.enabled\", \"true\")\n",
        "                  .config(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\")\n",
        "                  .config(\"spark.default.parallelism\",\"2\")\n",
        "                  .getOrCreate())\n",
        "\n",
        "spark.sparkContext.setLogLevel('INFO')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m8NPxx4AbC0"
      },
      "source": [
        "Let's load *Fake* and *True* datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crJYduCphW6r",
        "outputId": "e73782d2-6b01-4a8c-bd94-c97fc9fe06df"
      },
      "source": [
        "# True Dataset\n",
        "df_true = spark.read.csv('./fakenews/True.csv',header=True)\n",
        "df_true = df_true.withColumn(\"Label\",lit(\"True\")) # add a column named \"Label\" with all values as True\n",
        "print(\"There are\",df_true.count(),\"rows in the True dataset.\")\n",
        "# False Dataset\n",
        "df_false = spark.read.csv('./fakenews/Fake.csv',header=True)\n",
        "df_false = df_false.withColumn(\"Label\",lit(\"False\")) # add a column named \"Label\" with all values as False\n",
        "print(\"There are\",df_false.count(),\"rows in the False dataset.\")\n",
        "# Build a unique dataset\n",
        "df = df_true.union(df_false)\n",
        "\n",
        "# Transform date in the datetime format and remove the string column\n",
        "df = df.withColumn(\"Datetime\",to_date(df.date,\"MMMM dd, YYYY\")) # create a column named Datetime in datetime format\n",
        "df = df.drop(\"date\") # remove column date which contains date as strings\n",
        "count = df.count()\n",
        "print(\"In total, there are\",count,\"rows in the whole dataset.\")\n",
        "\n",
        "# Remove rows without title and text\n",
        "columns = ['text','title']\n",
        "df = df.na.drop(subset=columns) # remove rows where columns values are Nan\n",
        "print(count-df.count(),\"rows has been deleted because of Nan values in columns : \"+\" and \".join(columns),\"=>\",df.count(),\"rows in the dataset\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 21417 rows in the True dataset.\n",
            "There are 23489 rows in the False dataset.\n",
            "In total, there are 44906 rows in the whole dataset.\n",
            "8 rows has been deleted because of Nan values in columns : text and title => 44898 rows in the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEx5uakni-Yo",
        "outputId": "9001740d-5c30-4aff-9a92-167a677201a8"
      },
      "source": [
        "# A look at the dataframe schema\n",
        "df.printSchema()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- title: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- subject: string (nullable = true)\n",
            " |-- Label: string (nullable = false)\n",
            " |-- Datetime: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SedaXhkBWPIl"
      },
      "source": [
        "Let's take a look at *subjects*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj0RzO7CF1fI",
        "outputId": "09d0db21-de13-4e8d-ed06-1ad292412f20"
      },
      "source": [
        "df_tmp = df.groupBy('subject').count()\n",
        "df_tmp = df_tmp.sort(df_tmp['count'].desc())\n",
        "\n",
        "df_tmp.show()\n",
        "print(\"There are\",df_tmp.count(),'subjects in the dataset')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|             subject|count|\n",
            "+--------------------+-----+\n",
            "|        politicsNews|11209|\n",
            "|           worldnews|10115|\n",
            "|                News| 8501|\n",
            "|            politics| 6525|\n",
            "|           left-news| 4216|\n",
            "|     Government News| 1543|\n",
            "|             US_News|  767|\n",
            "|         Middle-east|  762|\n",
            "|     fjs);}(document|  124|\n",
            "|               2017\"|   26|\n",
            "|               2016\"|   18|\n",
            "| 2016Featured ima...|    8|\n",
            "| i'm sure it wasn...|    8|\n",
            "|              2017 \"|    7|\n",
            "| 2017Ranking memb...|    7|\n",
            "|                   s|    7|\n",
            "| and?\"\" he says. ...|    6|\n",
            "| make it into art...|    5|\n",
            "| fry em like baco...|    5|\n",
            "|      2017So all day|    4|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "There are 823 subjects int the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILutN7XlWUHp"
      },
      "source": [
        "As shown above some subjects seems corrupted and we will therefore keep only revelevant subjects (represented with more than 500 rows) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9oB6x2DljVw",
        "outputId": "cd0840b5-45d4-4441-bcae-5f5890ab2f4e"
      },
      "source": [
        "df_tmp = df.groupBy('subject').count().where(\"count>500\")\n",
        "df_tmp = df_tmp.sort(df_tmp['count'].desc())\n",
        "\n",
        "df_tmp.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----+\n",
            "|        subject|count|\n",
            "+---------------+-----+\n",
            "|   politicsNews|11209|\n",
            "|      worldnews|10115|\n",
            "|           News| 8501|\n",
            "|       politics| 6525|\n",
            "|      left-news| 4216|\n",
            "|Government News| 1543|\n",
            "|        US_News|  767|\n",
            "|    Middle-east|  762|\n",
            "+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1Xzb5ifXQS4"
      },
      "source": [
        "Let's remove corrupted subjects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWijOegwra1v",
        "outputId": "60483204-3fc7-470d-d072-66acc54e57c0"
      },
      "source": [
        "# Remove corrupted subjects\n",
        "count = df.count()\n",
        "subjects = [str(x.subject) for x in df_tmp.select('subject').collect()]\n",
        "res_df = df.where(df.subject.isin(subjects))\n",
        "    \n",
        "print(count-res_df.count(),\"rows has been deleted because of corrupted subjects =>\",res_df.count(),\"rows in the dataset\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1260 rows has been deleted because of corrupted subjects => 43638 rows in the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2kTfB4LEbjH"
      },
      "source": [
        "df = res_df"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Var_vcYfKl5P"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VqUO-HZXZls"
      },
      "source": [
        "Now our dataset is clean and ready for model building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFrxhNF-SQoZ",
        "outputId": "2ff22761-b94b-47dc-af6e-f56c5821aee0"
      },
      "source": [
        "# selection of feature/text and class/label\n",
        "df = df.select('text','Label')\n",
        "df.show(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|                text|Label|\n",
            "+--------------------+-----+\n",
            "|WASHINGTON (Reute...| True|\n",
            "|WASHINGTON (Reute...| True|\n",
            "|WASHINGTON (Reute...| True|\n",
            "|WASHINGTON (Reute...| True|\n",
            "|SEATTLE/WASHINGTO...| True|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdDjS1XTXuUN"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBVS1B1FKkoa"
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer,StopWordsRemover,CountVectorizer,IDF\n",
        "from pyspark.ml.feature import StringIndexer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpdihh-7Xw6L"
      },
      "source": [
        "Model skeleton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4oaxP5XMUak"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol='text',outputCol='mytokens')\n",
        "stopwords_remover = StopWordsRemover(inputCol='mytokens',outputCol='filtered_tokens')\n",
        "vectorizer = CountVectorizer(inputCol='filtered_tokens',outputCol='rawFeatures')\n",
        "idf = IDF(inputCol='rawFeatures',outputCol='vectorizedFeatures')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDJPZthxZpLB"
      },
      "source": [
        "Transform label column from string to float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er8a33vhNfTz",
        "outputId": "d780d279-f025-47cc-ee8c-c6fa202de99f"
      },
      "source": [
        "labelEncoder = StringIndexer(inputCol='Label',outputCol='label').fit(df)\n",
        "labelEncoder.transform(df).show(5)\n",
        "label_dict = {\"True\":1.0,\"False\":0.0}"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|                text|label|\n",
            "+--------------------+-----+\n",
            "|WASHINGTON (Reute...|  1.0|\n",
            "|WASHINGTON (Reute...|  1.0|\n",
            "|WASHINGTON (Reute...|  1.0|\n",
            "|WASHINGTON (Reute...|  1.0|\n",
            "|SEATTLE/WASHINGTO...|  1.0|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwQ_tz4nZ1CW"
      },
      "source": [
        "Transform the dataset label "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeoSHRUqNi8J"
      },
      "source": [
        "df = labelEncoder.transform(df)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCjGsd-2Z6ah"
      },
      "source": [
        "Split the dataset into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2irjPHVO1-8"
      },
      "source": [
        "trainDF,testDF = df.randomSplit((0.7,0.3),seed=42)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8kkoHI4aBKg"
      },
      "source": [
        "Import Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeIKQJLLTR3-"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni-VsOHKaHMD"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpLGCGwpTUCj"
      },
      "source": [
        "lr = LogisticRegression(featuresCol='vectorizedFeatures',labelCol='label')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsbsBgaEaJ2x"
      },
      "source": [
        "Import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP3y2lOqTWbk"
      },
      "source": [
        "from pyspark.ml import Pipeline"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtJIFnKhaMUy"
      },
      "source": [
        "Finally build the model pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFPUaYKFTZNG"
      },
      "source": [
        "pipeline = Pipeline(stages=[tokenizer,stopwords_remover,vectorizer,idf,lr])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR_fYuBead0F"
      },
      "source": [
        "Let's train the model on training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc0j0_mrTg3Y"
      },
      "source": [
        "lr_model = pipeline.fit(trainDF)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLPdOcDeaihO"
      },
      "source": [
        "Then predict on test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnTwRi7FTjz5",
        "outputId": "49476b85-dfb9-4398-90f4-2b41e969f22d"
      },
      "source": [
        "predictions = lr_model.transform(testDF)\n",
        "predictions.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|                text|label|            mytokens|     filtered_tokens|         rawFeatures|  vectorizedFeatures|       rawPrediction|         probability|prediction|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "| ((This December ...|  1.0|[, ((this, decemb...|[, ((this, decemb...|(262144,[0,1,3,4,...|(262144,[0,1,3,4,...|[-35.096204393460...|[5.72680105339041...|       1.0|\n",
            "| (Corrects Comey ...|  1.0|[, (corrects, com...|[, (corrects, com...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|[-34.956903176082...|[6.58278765780648...|       1.0|\n",
            "| (Corrects Feb. 2...|  1.0|[, (corrects, feb...|[, (corrects, feb...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|[-38.724634743910...|[1.52091455826497...|       1.0|\n",
            "| (Corrects first ...|  1.0|[, (corrects, fir...|[, (corrects, fir...|(262144,[0,2,7,9,...|(262144,[0,2,7,9,...|[-26.628580200255...|[2.72492502961918...|       1.0|\n",
            "| (Corrects paragr...|  1.0|[, (corrects, par...|[, (corrects, par...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|[-43.421446505481...|[1.38773130826740...|       1.0|\n",
            "| (Corrects spelli...|  1.0|[, (corrects, spe...|[, (corrects, spe...|(262144,[0,1,2,4,...|(262144,[0,1,2,4,...|[-26.560465886253...|[2.91699865464117...|       1.0|\n",
            "| (Corrects this A...|  1.0|[, (corrects, thi...|[, (corrects, aug...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|[-50.347986352858...|[1.36190668615183...|       1.0|\n",
            "| (Editors Note: A...|  1.0|[, (editors, note...|[, (editors, note...|(262144,[0,1,2,5,...|(262144,[0,1,2,5,...|[-28.220913915346...|[5.54386712126981...|       1.0|\n",
            "| (Editors note: A...|  1.0|[, (editors, note...|[, (editors, note...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|[-29.453127759497...|[1.61684779706166...|       1.0|\n",
            "| (Editorâ€™s note: ...|  1.0|[, (editorâ€™s, not...|[, (editorâ€™s, not...|(262144,[0,2,4,5,...|(262144,[0,2,4,5,...|[-32.907011576781...|[5.11289008206916...|       1.0|\n",
            "| (Editorâ€™s note: ...|  1.0|[, (editorâ€™s, not...|[, (editorâ€™s, not...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|[-3.0249429372258...|[0.04631167006905...|       1.0|\n",
            "| (In Dec. 25 stor...|  1.0|[, (in, dec., 25,...|[, (in, dec., 25,...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|[-22.644452711061...|[1.46433061320068...|       1.0|\n",
            "| (In March 13 ite...|  1.0|[, (in, march, 13...|[, (in, march, 13...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|[-67.206607357203...|[6.49443354700759...|       1.0|\n",
            "| (In March 16 ite...|  1.0|[, (in, march, 16...|[, (in, march, 16...|(262144,[0,2,5,7,...|(262144,[0,2,5,7,...|[-28.816288023736...|[3.05664584345659...|       1.0|\n",
            "| (In this Dec. 13...|  1.0|[, (in, this, dec...|[, (in, dec., 13,...|(262144,[0,1,2,5,...|(262144,[0,1,2,5,...|[-30.036169664281...|[9.02520878149683...|       1.0|\n",
            "| (In this Februar...|  1.0|[, (in, this, feb...|[, (in, february,...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|[-17.458816148410...|[2.61657070544790...|       1.0|\n",
            "| (In this Jan. 30...|  1.0|[, (in, this, jan...|[, (in, jan., 30,...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|[-51.182012944028...|[5.91472021176115...|       1.0|\n",
            "| (In this Oct. 13...|  1.0|[, (in, this, oct...|[, (in, oct., 13,...|(262144,[0,1,2,6,...|(262144,[0,1,2,6,...|[-12.372195664859...|[4.23469225499951...|       1.0|\n",
            "| (Please note str...|  1.0|[, (please, note,...|[, (please, note,...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|[-19.923109469973...|[2.22588895184513...|       1.0|\n",
            "| (Please note: St...|  1.0|[, (please, note:...|[, (please, note:...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|[-18.299266640987...|[1.12909236332380...|       1.0|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1FV8PH6aoqc"
      },
      "source": [
        "Import model evaluator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-heJEM4UDSR"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i64Qq160arm6"
      },
      "source": [
        "Declare evaluator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PLNvpEvUDOe"
      },
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo9ZJ9jUauM-"
      },
      "source": [
        "Compute the accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lMF1u3DUDKh",
        "outputId": "0db91ace-e193-467d-fe27-faa8bcdb75b1"
      },
      "source": [
        "accuracy = evaluator.evaluate(predictions)\n",
        "accuracy"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9923142613151152"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce8ZIIxmnlfu"
      },
      "source": [
        "### Hyperparameters and Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpErGH-razoG"
      },
      "source": [
        "Here we will use cross validation to do hyperparameter tuning and try to improve model performances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfP46YiVpZDP"
      },
      "source": [
        "# Import modules\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX2C9Sj5cgHA"
      },
      "source": [
        "Building the hyperparameter Grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ-DOQybUDGn"
      },
      "source": [
        "paramGrid = ParamGridBuilder()\\\n",
        "    .addGrid(vectorizer.minDF,[1.0,1.5])\\ # minDF hyperparameter of the vectorizer\n",
        "    .addGrid(lr.regParam,[0.1,0.01])\\ # reParam hyperparameter of the logistic regression\n",
        "    .build()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_pGkwzgcx3t"
      },
      "source": [
        "Creation of the cross validation with 2 folds (in real world it would be better to have 3 or more folds)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEOXSXQ0UDBv"
      },
      "source": [
        "crossVal = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=2)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh_eJ6l9dBpX"
      },
      "source": [
        "Launch the cross validation on training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WRF3T7YUC-i"
      },
      "source": [
        "cvModel = crossVal.fit(trainDF)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1VvCdFLdJ6Z"
      },
      "source": [
        "Get the accuracy of the best model in the cross validation/ hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmAbNB5iUCzU",
        "outputId": "2f0deca3-acc7-4020-d9f0-555119cf73c3"
      },
      "source": [
        "cvPrediction = cvModel.transform(testDF)\n",
        "cvModel.getEvaluator().evaluate(cvPrediction)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9955748777268846"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkC1acxOdsVE"
      },
      "source": [
        "<br><br><br><br><br><br><br><br><br><br><br>\n",
        "<h2> Done by Arthur Tanquerel Thouvenin <img src=\"https://media.giphy.com/media/dWlLf9EAC8u5Nd0ku4/giphy.gif\" width=\"50\"></h2>\n",
        "\n",
        "<img align='right' src=\"https://drive.google.com/uc?export=view&id=1EjS2yq_Onqz4gbyz-Xu9MvdP6arFvqX3\" width=\"130\">\n",
        "\n",
        "<p>\n",
        "  <em>\n",
        "    Data Scientist \n",
        "    <img src=\"https://media.giphy.com/media/QtOt8WyYCGQBiJJ4ZJ/giphy.gif\" width=\"15\">\n",
        "  </em>\n",
        "</p>\n",
        "\n",
        "[![Linkedin: thaianebraga](https://img.shields.io/badge/-Arthur_Tanquerel_Thouvenin-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/arthur-thouvenin-133822135/)](https://www.linkedin.com/in/arthur-thouvenin-133822135/)\n",
        "[![GitHub Thaiane](https://img.shields.io/github/followers/0AlphaZero0?label=follow&style=social)](https://github.com/0AlphaZero0)\n",
        "                        \n",
        "\n",
        "<h3> <img src=\"https://media.giphy.com/media/W7EAM6hdYrw7E3NBgE/giphy.gif\" width=\"30\"> If you want to know more about me, you are on the spot!</h3>\n",
        "\n",
        "\n",
        "- ðŸ”­ Iâ€™m currently working on personal projects involving photos of second hand items and AI processing\n",
        "- ðŸŒ± Iâ€™m currently learning AI, Data Science and much more\n",
        "- ðŸ”’ Ardent supporter of Open Source, Open Sciences, Open Data\n",
        "- ðŸ’¬ Ask me about Persistent Identifiers (ROR IDs), Biomedical Litterature, NLP\n",
        "- ðŸ“« How to reach me: athouvenin [at] outlook.com\n",
        "- ðŸ˜„ Pronouns: He/Him\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=center>\n",
        "    <img height=160 align=\"center\" src=\"https://github-readme-stats.vercel.app/api?username=0AlphaZero0&show_icons=true&theme=midnight-purple\">\n",
        "    <img height=160 align=\"center\" src=\"https://github-readme-stats.vercel.app/api/top-langs/?username=0AlphaZero0&layout=compact&theme=midnight-purple\">\n",
        "</p>\n",
        "\n",
        "\n",
        "<!--START_SECTION:badges-->\n",
        "<!--END_SECTION:badges-->\n",
        "\n",
        "\n",
        "<h3> May the Force be with you !</h3>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.giphy.com/media/GExBk9r9lP9LN5j2H5/giphy.gif\">\n",
        "</p>\n",
        "\n",
        "<br><p align=\"right\">![](https://visitor-badge.laobi.icu/badge?page_id=0ALphaZero0.0AlphaZero0)<br>\n"
      ]
    }
  ]
}